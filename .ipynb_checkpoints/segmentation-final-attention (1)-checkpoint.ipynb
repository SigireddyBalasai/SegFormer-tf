{"cells":[{"cell_type":"markdown","metadata":{"id":"SVJ5TCtWPZ68"},"source":["# Semantic segmentation with SegFormer and Hugging Face Transformers\n","\n","**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n","**Date created:** 2023/01/25<br>\n","**Last modified:** 2023/01/29<br>\n","**Description:** Fine-tuning a SegFormer model variant for semantic segmentation."]},{"cell_type":"markdown","metadata":{"id":"bpMgX0FzPZ6-"},"source":["## Introduction\n","\n","In this example, we show how to fine-tune a SegFormer model variant to do\n","semantic segmentation on a custom dataset. Semantic segmentation is the task of\n","assigning a category to each and every pixel of an image. SegFormer was proposed in\n","[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203).\n","SegFormer uses a hierarchical Transformer architecture (called \"Mix Transformer\") as\n","its encoder and a lightweight decoder for segmentation. As a result, it yields\n","state-of-the-art performance on semantic segmentation while being more efficient than\n","existing models. For more details, check out the original paper.\n","\n","![segformer-arch](https://i.imgur.com/BsrVwYe.png)\n","\n","We leverage\n","[Hugging Face Transformers](https://github.com/huggingface/transformers)\n","to load a pretrained SegFormer checkpoint and fine-tune it on a custom dataset.\n","\n","**Note:** this example reuses code from the following sources:\n","\n","* [Official tutorial on segmentation from the TensorFlow team](https://www.tensorflow.org/tutorials/images/segmentation)\n","* [Hugging Face Task guide on segmentation](https://huggingface.co/docs/transformers/main/en/tasks/semantic_segmentation)\n","\n","To run this example, we need to install the `transformers` library:"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T11:22:38.397235Z","iopub.status.busy":"2024-05-04T11:22:38.396826Z","iopub.status.idle":"2024-05-04T11:22:49.936755Z","shell.execute_reply":"2024-05-04T11:22:49.935807Z","shell.execute_reply.started":"2024-05-04T11:22:38.397204Z"},"id":"V2j5Q643PZ6_","trusted":true},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["!!pip install transformers -q"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T11:22:49.938793Z","iopub.status.busy":"2024-05-04T11:22:49.938471Z","iopub.status.idle":"2024-05-04T11:23:03.587367Z","shell.execute_reply":"2024-05-04T11:23:03.586181Z","shell.execute_reply.started":"2024-05-04T11:22:49.938767Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: roboflow in /opt/conda/lib/python3.10/site-packages (1.1.28)\n","Requirement already satisfied: certifi==2023.7.22 in /opt/conda/lib/python3.10/site-packages (from roboflow) (2023.7.22)\n","Requirement already satisfied: chardet==4.0.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.0.0)\n","Requirement already satisfied: cycler==0.10.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.10.0)\n","Requirement already satisfied: idna==2.10 in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.10)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.4.5)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from roboflow) (3.7.4)\n","Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.24.4)\n","Requirement already satisfied: opencv-python-headless==4.8.0.74 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.8.0.74)\n","Requirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from roboflow) (9.5.0)\n","Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.8.2)\n","Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.0.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.31.0)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.16.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.26.18)\n","Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.66.1)\n","Requirement already satisfied: PyYAML>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (6.0.1)\n","Requirement already satisfied: requests-toolbelt in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.10.1)\n","Requirement already satisfied: python-magic in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.4.27)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (1.2.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (4.47.0)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (21.3)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->roboflow) (3.3.2)\n","loading Roboflow workspace...\n","loading Roboflow project...\n"]}],"source":["!pip install roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"ihYIM7BFL7rugXS2qI8E\")\n","project = rf.workspace(\"balasai\").project(\"jhv-wp8qu\")\n","dataset = project.version(6).download(\"coco-segmentation\")\n"]},{"cell_type":"markdown","metadata":{"id":"A_ZBGQDNPZ7A"},"source":["## Load the data\n","\n","We use the [Oxford-IIIT Pets](https://www.robots.ox.ac.uk/~vgg/data/pets/) dataset for\n","this example. We leverage `tensorflow_datasets` to load the dataset."]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T11:23:03.589478Z","iopub.status.busy":"2024-05-04T11:23:03.589121Z","iopub.status.idle":"2024-05-04T11:23:03.596177Z","shell.execute_reply":"2024-05-04T11:23:03.595144Z","shell.execute_reply.started":"2024-05-04T11:23:03.589447Z"},"id":"5c5TkFA0PZ7B","trusted":true},"outputs":[{"data":{"text/plain":["'import tensorflow_datasets as tfds\\n\\ndataset, info = tfds.load(\"oxford_iiit_pet:3.*.*\", with_info=True)'"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["'''import tensorflow_datasets as tfds\n","\n","dataset, info = tfds.load(\"oxford_iiit_pet:3.*.*\", with_info=True)'''"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T11:23:03.599887Z","iopub.status.busy":"2024-05-04T11:23:03.599202Z","iopub.status.idle":"2024-05-04T11:23:16.053136Z","shell.execute_reply":"2024-05-04T11:23:16.051635Z","shell.execute_reply.started":"2024-05-04T11:23:03.599849Z"},"trusted":true},"outputs":[],"source":["!pip install pycocotools -q"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T11:23:16.055595Z","iopub.status.busy":"2024-05-04T11:23:16.055226Z","iopub.status.idle":"2024-05-04T11:23:16.196945Z","shell.execute_reply":"2024-05-04T11:23:16.195983Z","shell.execute_reply.started":"2024-05-04T11:23:16.055563Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.02s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","{'file_name': <tf.Tensor: shape=(), dtype=string, numpy=b'Valdes_SRW_WV2_PS_20120919_B17_PNG.rf.de81e9c1e86f53f35447e606e0310529.jpg'>, 'image': <tf.Tensor: shape=(512, 512, 3), dtype=uint8, numpy=\n","array([[[  0,   0,   0],\n","        [255, 255, 253],\n","        [251, 251, 249],\n","        ...,\n","        [252, 254, 249],\n","        [252, 254, 249],\n","        [252, 254, 249]],\n","\n","       [[ 19,  19,  17],\n","        [231, 231, 229],\n","        [255, 255, 253],\n","        ...,\n","        [254, 255, 251],\n","        [254, 255, 251],\n","        [254, 255, 251]],\n","\n","       [[  0,   0,   0],\n","        [  9,   9,   7],\n","        [  0,   0,   0],\n","        ...,\n","        [  0,   1,   0],\n","        [  0,   1,   0],\n","        [  0,   1,   0]],\n","\n","       ...,\n","\n","       [[  0,   2,   0],\n","        [  6,  11,   5],\n","        [  0,   2,   0],\n","        ...,\n","        [  0,   0,   0],\n","        [  0,   1,   0],\n","        [  0,   1,   0]],\n","\n","       [[ 15,  22,  15],\n","        [227, 234, 227],\n","        [251, 255, 251],\n","        ...,\n","        [255, 255, 253],\n","        [254, 255, 253],\n","        [254, 255, 253]],\n","\n","       [[  0,   3,   0],\n","        [251, 255, 251],\n","        [247, 254, 247],\n","        ...,\n","        [253, 253, 251],\n","        [252, 254, 251],\n","        [252, 254, 251]]], dtype=uint8)>, 'label': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, 'segmentation_mask': <tf.Tensor: shape=(512, 512, 1), dtype=uint8, numpy=\n","array([[[0],\n","        [0],\n","        [0],\n","        ...,\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        ...,\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        ...,\n","        [0],\n","        [0],\n","        [0]],\n","\n","       ...,\n","\n","       [[0],\n","        [0],\n","        [0],\n","        ...,\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        ...,\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        ...,\n","        [0],\n","        [0],\n","        [0]]], dtype=uint8)>}\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","from pycocotools.coco import COCO\n","from typing import List, Dict, Any, Tuple\n","\n","# Load COCO annotations (replace 'your_annotations_file.json' with your actual file path)\n","coco_train: COCO = COCO('/kaggle/working/jhv-6/train/_annotations.coco.json')\n","coco_valid: COCO = COCO('/kaggle/working/jhv-6/valid/_annotations.coco.json')    \n","\n","# Function to generate mask using coco.annToMask\n","def generate_mask(annotations: List[Dict[str, Any]], image_shape: Tuple[int, int],coco) -> np.ndarray:\n","    combined_mask: np.ndarray = np.zeros(image_shape[:2], dtype=np.uint8)\n","    for ann in annotations:\n","        mask = coco.annToMask(ann)\n","        combined_mask += mask\n","    return combined_mask\n","\n","# Generator function to yield data\n","def data_generator(coco: COCO, image_dir: str):\n","    for image_id in coco.imgs:\n","        image_info: Dict[str, Any] = coco.loadImgs(image_id)[0]\n","        file_name: str = image_info['file_name']\n","        image_path: str = f\"{image_dir}/{file_name}\"\n","\n","        # Load the actual image using TensorFlow\n","        image: tf.Tensor = tf.io.read_file(image_path)\n","        image: tf.Tensor = tf.image.decode_image(image, channels=3)\n","\n","        # Load the segmentation mask (modify as per your annotation format)\n","        annotations_ids: List[int] = coco.getAnnIds(imgIds=image_id)\n","        annotations: List[Dict[str, Any]] = coco.loadAnns(annotations_ids)\n","\n","        # Extracting labels from annotations (example assumes 'category_id' is used for labels)\n","        labels: List[int] = [ann['category_id'] for ann in annotations]\n","\n","        # Generate mask using coco.annToMask\n","        mask: np.ndarray = generate_mask(annotations, image.numpy().shape,coco)\n","        mask = tf.expand_dims(mask,axis=-1)\n","        yield {\n","            'file_name': file_name,\n","            'image': image.numpy(),  # Convert to numpy array\n","            'label': labels,\n","            'segmentation_mask': mask\n","        }\n","\n","# Provide the directory where images are stored\n","train_directory: str = '/kaggle/working/jhv-6/train/'\n","valid_directory: str = '/kaggle/working/jhv-6/valid/'    \n","\n","# Create the TensorFlow dataset using from_generator\n","train_dataset = tf.data.Dataset.from_generator(\n","    generator=lambda: data_generator(coco_train, train_directory),\n","    output_signature={\n","        'file_name': tf.TensorSpec(shape=(), dtype=tf.string),\n","        'image': tf.TensorSpec(shape=(None, None, 3), dtype=tf.uint8),\n","        'label': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n","        'segmentation_mask': tf.TensorSpec(shape=(None, None,1), dtype=tf.uint8),\n","    }\n",")\n","valid_dataset = tf.data.Dataset.from_generator(\n","    generator=lambda: data_generator(coco_valid, valid_directory),\n","    output_signature={\n","        'file_name': tf.TensorSpec(shape=(), dtype=tf.string),\n","        'image': tf.TensorSpec(shape=(None, None, 3), dtype=tf.uint8),\n","        'label': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n","        'segmentation_mask': tf.TensorSpec(shape=(None, None,1), dtype=tf.uint8),\n","    }\n",")\n","# Print a sample from the dataset\n","for sample in train_dataset.take(1):\n","    print(sample)"]},{"cell_type":"markdown","metadata":{"id":"1w27_69-PZ7B"},"source":["## Prepare the datasets\n","\n","For preparing the datasets for training and evaluation, we:\n","\n","* Normalize the images with the mean and standard deviation used during pre-training\n","SegFormer.\n","* Subtract 1 from the segmentation masks so that the pixel values start from 0.\n","* Resize the images.\n","* Transpose the images such that they are in `\"channels_first\"` format. This is to make\n","them compatible with the SegFormer model from Hugging Face Transformers."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T11:23:16.199180Z","iopub.status.busy":"2024-05-04T11:23:16.198366Z","iopub.status.idle":"2024-05-04T11:23:16.211674Z","shell.execute_reply":"2024-05-04T11:23:16.210512Z","shell.execute_reply.started":"2024-05-04T11:23:16.199153Z"},"id":"fxmBP54uPZ7B","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import backend\n","\n","image_size = 512\n","mean = tf.constant([0.485, 0.456, 0.406])\n","std = tf.constant([0.229, 0.512, 0.225])\n","\n","\n","def normalize(input_image, input_mask):\n","    input_image = tf.image.convert_image_dtype(input_image, tf.float32)\n","    input_image = (input_image - mean) / tf.maximum(std, backend.epsilon())\n","    #input_mask -= 1\n","    return input_image, input_mask\n","\n","\n","def load_image(datapoint):\n","    print(datapoint)\n","    print(datapoint['image'],datapoint['segmentation_mask'])\n","    input_image = tf.image.resize(datapoint[\"image\"], (image_size, image_size))\n","    input_mask = tf.image.resize(\n","        datapoint[\"segmentation_mask\"],\n","        (image_size, image_size),\n","        method=\"bilinear\",\n","    )\n","\n","    input_image, input_mask = normalize(input_image, input_mask)\n","    #input_image = tf.transpose(input_image, (2, 0, 1))\n","    return ({\"pixel_values\": input_image,},tf.squeeze(input_mask))\n"]},{"cell_type":"markdown","metadata":{"id":"nLyrwDyKPZ7B"},"source":["We now use the above utilities to prepare `tf.data.Dataset` objects including\n","`prefetch()` for performance. Change the `batch_size` to match the size of the GPU memory\n","on the GPU that you're using for training."]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T11:23:16.213524Z","iopub.status.busy":"2024-05-04T11:23:16.213028Z","iopub.status.idle":"2024-05-04T11:23:16.344290Z","shell.execute_reply":"2024-05-04T11:23:16.343247Z","shell.execute_reply.started":"2024-05-04T11:23:16.213478Z"},"id":"7jLt9-eoPZ7B","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'file_name': <tf.Tensor 'args_0:0' shape=() dtype=string>, 'image': <tf.Tensor 'args_1:0' shape=(None, None, 3) dtype=uint8>, 'label': <tf.Tensor 'args_2:0' shape=(None,) dtype=int32>, 'segmentation_mask': <tf.Tensor 'args_3:0' shape=(None, None, 1) dtype=uint8>}\n","Tensor(\"args_1:0\", shape=(None, None, 3), dtype=uint8) Tensor(\"args_3:0\", shape=(None, None, 1), dtype=uint8)\n","{'file_name': <tf.Tensor 'args_0:0' shape=() dtype=string>, 'image': <tf.Tensor 'args_1:0' shape=(None, None, 3) dtype=uint8>, 'label': <tf.Tensor 'args_2:0' shape=(None,) dtype=int32>, 'segmentation_mask': <tf.Tensor 'args_3:0' shape=(None, None, 1) dtype=uint8>}\n","Tensor(\"args_1:0\", shape=(None, None, 3), dtype=uint8) Tensor(\"args_3:0\", shape=(None, None, 1), dtype=uint8)\n"]}],"source":["auto = tf.data.AUTOTUNE\n","batch_size = 4\n","\n","train_ds = (\n","    train_dataset\n","    .cache()\n","    .shuffle(batch_size * 10)\n","    .map(load_image, num_parallel_calls=auto)\n","    .batch(batch_size)\n","    .prefetch(auto)\n",")\n","test_ds = (\n","    valid_dataset\n","    .map(load_image, num_parallel_calls=auto)\n","    .batch(batch_size)\n","    .prefetch(auto)\n",")"]},{"cell_type":"markdown","metadata":{"id":"JZD6hYLyPZ7B"},"source":["We can check the shapes of the input images and their segmentation maps:"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T11:23:16.346019Z","iopub.status.busy":"2024-05-04T11:23:16.345658Z","iopub.status.idle":"2024-05-04T11:23:16.351389Z","shell.execute_reply":"2024-05-04T11:23:16.350346Z","shell.execute_reply.started":"2024-05-04T11:23:16.345984Z"},"id":"h0SwcqOBPZ7C","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["({'pixel_values': TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=None)}, TensorSpec(shape=(None, 512, 512), dtype=tf.float32, name=None))\n"]}],"source":["print(train_ds.element_spec)"]},{"cell_type":"markdown","metadata":{"id":"hWbcBR7qPZ7C"},"source":["## Visualize dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T11:23:16.353487Z","iopub.status.busy":"2024-05-04T11:23:16.352612Z"},"id":"jALy75EqPZ7C","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","\n","def display(display_list):\n","    plt.figure(figsize=(15, 15))\n","\n","    title = [\"Input Image\", \"True Mask\", \"Predicted Mask\"]\n","\n","    for i in range(len(display_list)):\n","        plt.subplot(1, len(display_list), i + 1)\n","        plt.title(title[i])\n","        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n","        plt.axis(\"off\")\n","    plt.show()\n","\n","\n","for samples,labels in train_ds.take(2):\n","    sample_image, sample_mask = samples[\"pixel_values\"][0], labels[0]\n","    #sample_image = tf.transpose(sample_image, (1, 2, 0))\n","    sample_mask = tf.expand_dims(sample_mask, -1)\n","    display([sample_image, sample_mask])"]},{"cell_type":"markdown","metadata":{"id":"XBw_Nm-kPZ7C"},"source":["## Load a pretrained SegFormer checkpoint\n","\n","We now load a pretrained SegFormer model variant from Hugging Face Transformers. The\n","SegFormer model comes in different variants dubbed as **MiT-B0** to **MiT-B5**. You can\n","find these checkpoints\n","[here](https://huggingface.co/models?pipeline_tag=image-segmentation&sort=downloads&search=segformer).\n","We load the smallest variant Mix-B0, which produces a good trade-off\n","between inference efficiency and predictive performance."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from models import SegFormer_B3\n","model = SegFormer_B3(input_shape = (512, 512, 3), num_classes = 1)\n","print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_image.shape\n","from matplotlib import pyplot as plt\n","plt.imshow(tf.keras.utils.array_to_img(sample_image))\n","model.predict(tf.expand_dims(sample_image,axis=0))"]},{"cell_type":"markdown","metadata":{"id":"d5eCxRYbPZ7C"},"source":["## Compile the model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def custom_loss(logits, labels):\n","    # `labels` is of shape (batch_size, height, width)\n","    # logits are predicted as (batch_size, classes, height//2, width//2)\n","    #logits = tf.transpose(logits, [0, 2, 3, 1])\n","    label_interp_shape = labels.shape[1:]\n","    upsampled_logits = tf.image.resize(logits, size=label_interp_shape, method=\"bilinear\")\n","    \n","    loss_fct = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True,\n","        reduction=\"none\",\n","    )\n","    \n","    def masked_loss(real, pred):\n","        label_weights = tf.one_hot(real, len(weights))*constant_weights\n","        label_weights = tf.reduce_sum(label_weights, axis=-1)\n","        unmasked_loss = loss_fct(real, pred)\n","        unmasked_loss *= label_weights\n","        mask = tf.cast(real != model.config.semantic_loss_ignore_index, dtype=unmasked_loss.dtype)\n","        masked_loss = unmasked_loss * mask\n","        reduced_masked_loss = tf.reduce_sum(masked_loss) /  tf.reduce_sum(mask)\n","        return tf.reshape(reduced_masked_loss, (1,))\n","\n","    return masked_loss(labels, upsampled_logits)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers.keras_callbacks import KerasMetricCallback\n","\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    # logits are of shape (batch_size, num_labels, height, width), so\n","    # we first transpose them to (batch_size, height, width, num_labels)\n","    logits = tf.transpose(logits, perm=[0, 2, 3, 1])\n","    # scale the logits to the size of the label\n","    logits_resized = tf.image.resize(\n","        logits,\n","        size=tf.shape(labels)[1:],\n","        method=\"bilinear\",\n","    )\n","    # compute the prediction labels and compute the metric\n","    pred_labels = tf.argmax(logits_resized, axis=-1)\n","    metrics = metric.compute(\n","        predictions=pred_labels,\n","        references=labels,\n","        num_labels=num_labels,\n","        ignore_index=-1,\n","        reduce_labels=image_processor.do_reduce_labels,\n","    )\n","    # add per category metrics as individual key-value pairs\n","    per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n","    per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n","\n","    metrics.update(\n","        {f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)}\n","    )\n","    metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n","    return {\"val_\" + k: v for k, v in metrics.items()}\n","\n","\n","metric_callback = KerasMetricCallback(\n","    metric_fn=compute_metrics,\n","    eval_dataset=test_ds,\n","    batch_size=batch_size,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mSzfCBtPZ7C","trusted":true},"outputs":[],"source":["lr = 0.00006\n","optimizer = tf.keras.optimizers.AdamW(learning_rate=lr,weight_decay=0.0006)\n","model.compile(optimizer=optimizer,loss=custom_loss)"]},{"cell_type":"markdown","metadata":{"id":"d-BX4-tDPZ7D"},"source":["Notice that we are not using any loss function for compiling the model. This is because\n","the forward pass of the model\n","[implements](https://github.com/huggingface/transformers/blob/820c46a707ddd033975bc3b0549eea200e64c7da/src/transformers/models/segformer/modeling_tf_segformer.py#L873)\n","the loss computation part when we provide labels alongside the input images. After\n","computing the loss, the model returned a structured `dataclass` object which is\n","then used to guide the training process.\n","\n","With the compiled model, we can proceed and call `fit()` on it to begin the fine-tuning\n","process!"]},{"cell_type":"markdown","metadata":{"id":"hNtynYmRPZ7D"},"source":["## Prediction callback to monitor training progress\n","\n","It helps us to visualize some sample predictions when the model is being fine-tuned,\n","thereby helping us to monitor the progress of the model. This callback is inspired from\n","[this tutorial](https://www.tensorflow.org/tutorials/images/segmentation)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBcIZZx2PZ7D","trusted":true},"outputs":[],"source":["from IPython.display import clear_output\n","\n","\n","def create_mask(pred_mask):\n","   return pred_mask[0]\n","\n","\n","def show_predictions(dataset=None, num=1):\n","    if dataset:\n","        dataset=dataset.shuffle(20)\n","        for sample,labels in dataset.take(num):\n","            images, masks = sample[\"pixel_values\"], labels\n","            masks = tf.expand_dims(masks, -1)\n","            pred_masks = model.predict(images)\n","            print(pred_masks.shape)\n","            #images = tf.transpose(images, (0, 2, 3, 1))\n","            display([images[0], masks[0], create_mask(pred_masks)])\n","    else:\n","        display(\n","            [\n","                sample_image,\n","                sample_mask,\n","                create_mask(model.predict(tf.expand_dims(sample_image, 0))),\n","            ]\n","        )\n","\n","\n","class DisplayCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, dataset, **kwargs):\n","        super().__init__(**kwargs)\n","        self.dataset = dataset\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        clear_output(wait=True)\n","        show_predictions(self.dataset)\n","        print(\"\\nSample Prediction after epoch {}\\n\".format(epoch + 1))\n"]},{"cell_type":"markdown","metadata":{"id":"V6ULC51NPZ7D"},"source":["## Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install keras-tuner --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GxiK2EuPZ7D","trusted":true},"outputs":[],"source":["# Increase the number of epochs if the results are not of expected quality.\n","epochs = 50\n","\n","history = model.fit(\n","    train_ds,\n","    validation_data=test_ds,\n","    callbacks=[DisplayCallback(test_ds),tf.keras.callbacks.ReduceLROnPlateau(min_lr=0.0),metric_callback],\n","    epochs=epochs,\n",")"]},{"cell_type":"markdown","metadata":{"id":"QtZNTYj1PZ7D"},"source":["## Inference\n","\n","We perform inference on a few samples from the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjo_rWj7PZ7D","trusted":true},"outputs":[],"source":["show_predictions(test_ds, 5)"]},{"cell_type":"markdown","metadata":{"id":"R5XFDmXbPZ7D"},"source":["## Conclusion\n","\n","In this example, we learned how to fine-tune a SegFormer model variant on a custom\n","dataset for semantic segmentation. In the interest of brevity, the example\n","was kept short. However, there are a couple of things, you can further try out:\n","\n","* Incorporate data augmentation to potentially improve the results.\n","* Use a larger SegFormer model checkpoint to see how the results are affected.\n","* Push the fine-tuned model to the Hugging Face for sharing with the community easily.\n","You can do so just by doing `model.push_to_hub(\"your-username/your-awesome-model\")`.\n","And then you can load the model by doing\n","`TFSegformerForSemanticSegmentation.from_pretrained(\"your-username/your-awesome-model\"`).\n","[Here](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb)\n","is an end-to-end example if you're looking for a reference.\n","* If you'd rather push the model checkpoints to the Hub as the model is being\n","fine-tuned you can instead use the `PushToHubCallback` Keras callback.\n","[Here](https://gist.github.com/sayakpaul/f474ffb01f0cdcc8ba239357965c3bca) is an example.\n","[Here](https://huggingface.co/sayakpaul/mit-b0-finetuned-pets) is an example of a model\n","repository that was created using this callback."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"segformer","provenance":[],"toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
